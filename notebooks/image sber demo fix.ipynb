{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6951e3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "CODE_DIR = '../../sber-swap'\n",
    "os.chdir(f'./{CODE_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "930153e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/kornia/augmentation/augmentation.py:1830: DeprecationWarning: GaussianBlur is no longer maintained and will be removed from the future versions. Please use RandomGaussianBlur instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "\n",
    "from utils.inference.image_processing import crop_face, get_final_image, show_images\n",
    "from utils.inference.video_processing import read_video, get_target, get_final_video, add_audio_from_another_video, face_enhancement\n",
    "from utils.inference.core import model_inference\n",
    "\n",
    "from network.AEI_Net import AEI_Net\n",
    "from coordinate_reg.image_infer import Handler\n",
    "from insightface_func.face_detect_crop_multi import Face_detect_crop\n",
    "from arcface_model.iresnet import iresnet100\n",
    "from models.pix2pix_model import Pix2PixModel\n",
    "from models.config_sr import TestOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d7fdc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecda288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3b63a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(use_sr=False):\n",
    "    app = Face_detect_crop(name='antelope', root='./insightface_func/models')\n",
    "    app.prepare(ctx_id= 0, det_thresh=0.6, det_size=(640,640))\n",
    "\n",
    "    # main model for generation\n",
    "    G = AEI_Net(backbone='unet', num_blocks=2, c_id=512)\n",
    "    G.eval()\n",
    "    G.load_state_dict(torch.load('weights/G_unet_2blocks.pth', map_location=torch.device('cpu')))\n",
    "    G = G.cuda()\n",
    "    G = G.half()\n",
    "\n",
    "    # arcface model to get face embedding\n",
    "    netArc = iresnet100(fp16=False)\n",
    "    netArc.load_state_dict(torch.load('arcface_model/backbone.pth'))\n",
    "    netArc=netArc.cuda()\n",
    "    netArc.eval()\n",
    "\n",
    "    # model to get face landmarks\n",
    "    handler = Handler('./coordinate_reg/model/2d106det', 0, ctx_id=0, det_size=640)\n",
    "\n",
    "    # model to make superres of face, set use_sr=True if you want to use super resolution or use_sr=False if you don't\n",
    "    model = None\n",
    "    if use_sr:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        opt = TestOptions()\n",
    "        #opt.which_epoch ='10_7'\n",
    "        model = Pix2PixModel(opt)\n",
    "        model.netG.train()\n",
    "    \n",
    "    return app, G, netArc, handler, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d678c31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_image = True\n",
    "aligned = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1e5afed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(source_full, target_full, use_sr, aligned=False,):\n",
    "    # check, if we can detect face on the source image\n",
    "    crop_size = 224 # don't change this\n",
    "    BS = 60\n",
    "    app, G, netArc, handler, model = load_model(use_sr=True)\n",
    "    print(source_full.shape)\n",
    "    try:\n",
    "        if not aligned:\n",
    "            source = crop_face(source_full, app, crop_size)[0]\n",
    "            source = [source[:, :, ::-1]]\n",
    "        else:\n",
    "            if source_full.shape[0] > 224:\n",
    "                source = [cv2.resize(source_full, (224, 224))[:, :, ::-1]]\n",
    "            else:\n",
    "                source = [source_full]\n",
    "        print(\"Everything is ok!\")\n",
    "    except TypeError:\n",
    "        print(\"Bad source images\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.rcParams[\"figure.figsize\"] = (30,30)\n",
    "    plt.imshow(cv2.resize(source_full, (224, 224)))\n",
    "    plt.show()\n",
    "    full_frames = [target_full]\n",
    "    target = get_target(full_frames, app, crop_size)\n",
    "\n",
    "        \n",
    "    final_frames_list, crop_frames_list, full_frames, tfm_array_list = model_inference(full_frames,\n",
    "                                                                                       source,\n",
    "                                                                                       target,\n",
    "                                                                                       netArc,\n",
    "                                                                                       G,\n",
    "                                                                                       app,\n",
    "                                                                                       set_target = False,\n",
    "                                                                                       crop_size=crop_size,\n",
    "                                                                                       BS=BS)\n",
    "    if use_sr:\n",
    "        final_frames_list = face_enhancement(final_frames_list, model)\n",
    "    result = get_final_image(final_frames_list, crop_frames_list, full_frames[0], tfm_array_list, handler)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "271483a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# gr.Interface(fn=swap_image, inputs=[\"image\", \"image\", \"text\", \"text\", 'text'], outputs=\"image\").launch(server_name='0.0.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fc2974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    ['examples/images/320_persona.png', 'examples/images/sample_dst.jpg', True, True],\n",
    "    ['examples/images/src1.png', 'examples/images/dst1.jpg', True, True],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85bb54ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/gradio/inputs.py:270: DeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/gradio/deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "/opt/conda/lib/python3.8/site-packages/gradio/inputs.py:270: DeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/gradio/outputs.py:44: DeprecationWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://localhost:7860/\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"500\"\n",
       "            src=\"http://localhost:7860/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f1b29634790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<gradio.routes.App at 0x7f1b295caf70>, 'http://localhost:7860/', None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.Interface(\n",
    "    run_inference,\n",
    "    inputs=[\n",
    "        gr.inputs.Image(),\n",
    "        gr.inputs.Image(),\n",
    "        gr.Checkbox(),\n",
    "        gr.Checkbox(),\n",
    "    ],\n",
    "    outputs=gr.outputs.Image(),\n",
    "    examples=examples\n",
    ").launch(server_name='0.0.0.0')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f94858e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
